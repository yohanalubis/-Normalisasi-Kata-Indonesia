{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sqlite3\n",
    "import io\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Menghubungkan ke database\n",
    "from sqlite3 import Error\n",
    "db_file = 'DBnews.db'\n",
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(db_file)\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mengambil kolom comments untuk diolah\n",
    "comments = []\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * from t_comments\")\n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    comments.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mengambil hanya konten comments\n",
    "new_comments = []\n",
    "for comment in comments:\n",
    "    new_comments.append(comment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing data (removePunctuation dan lowercase)\n",
    "\n",
    "def removePunctuation(x):\n",
    "    # Lowercasing all words\n",
    "    x = x.lower()\n",
    "    # Removing non ASCII chars\n",
    "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n",
    "    # Removing (replacing with empty spaces actually) all the punctuations\n",
    "    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n",
    "\n",
    "pre_comments = []\n",
    "for comment in new_comments:\n",
    "    pre_comments.append(removePunctuation(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenisasi hasil preprocessing data\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tok_comments = []\n",
    "for comment in low_comments:\n",
    "    tok_comments.append(tokenizer.tokenize(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Menghilangkan token yang kosong\n",
    "for comment in tok_comments:\n",
    "    for sub_comment in comment:\n",
    "        if sub_comment == '':\n",
    "            comment.remove(sub_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Untuk mempercepat proses, gunakan 1000 data\n",
    "tok_comments = tok_comments[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mengolah date menggunakan corpus yang sudah ada\n",
    "start_time = time.time()\n",
    "print('Streaming wiki...')\n",
    "id_wiki = gensim.corpora.WikiCorpus('idwiki-latest-pages-articles.xml.bz2', lemmatize=False, dictionary={})\n",
    "article_count = 0\n",
    "with io.open('idwiki.txt', 'w', encoding='utf-8') as wiki_txt:\n",
    "    for text in id_wiki.get_texts():\n",
    "        wiki_txt.write(\" \".join(text) + '\\n')\n",
    "        article_count += 1\n",
    "        if article_count % 10000 == 0:\n",
    "            print('{} articles processed'.format(article_count))\n",
    "    print('total: {} articles'.format(article_count))\n",
    "finish_time = time.time()\n",
    "print('Elapsed time: {}'.format(timedelta(seconds=finish_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pembuatan model word2vec\n",
    "start_time = time.time()\n",
    "print('Training Word2Vec Model...')\n",
    "sentences = word2vec.LineSentence('idwiki.txt')\n",
    "id_w2v = word2vec.Word2Vec(sentences, size=200, workers=multiprocessing.cpu_count()-1)\n",
    "id_w2v.save('idwiki_word2vec.model')\n",
    "finish_time = time.time()\n",
    "print('Finished. Elapsed time: {}'.format(timedelta(seconds=finish_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cheking the model\n",
    "id_w2v.wv.most_similar(\"tdk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Memproses data slang/alay\n",
    "import pandas as pd\n",
    "slang_corpus = pd.read_csv(\"colloquial-indonesian-lexicon.csv\")\n",
    "slang_corpus = slang_corpus[slang_corpus.columns[0:2]]\n",
    "slang_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Menggunakan perbandingan dengan word2vec yang ada dan edit distance untuk membuat kata menjadi kata formal\n",
    "print(\"Processing data...\")\n",
    "start_time = time.time()\n",
    "import nltk\n",
    "corr_comments = []\n",
    "slang_words = []\n",
    "reco_words = []\n",
    "corr_word = 0\n",
    "tot_word = 0\n",
    "for comment in tok_comments:\n",
    "    corr_comment = []\n",
    "    slang_word = []\n",
    "    for sub_comment in comment:\n",
    "        if sub_comment in id_w2v.wv.vocab:\n",
    "            sim_slang = slang_corpus[slang_corpus['slang'] == sub_comment]\n",
    "            if len(sim_slang) != 0:\n",
    "                mode_word = sim_slang.formal.mode()[0]\n",
    "                corr_word = corr_word + 1\n",
    "                corr_comment.append(mode_word)\n",
    "            else:\n",
    "                corr_word = corr_word + 1\n",
    "                corr_comment.append(sub_comment)\n",
    "        else:\n",
    "            comment_found = False\n",
    "            sim_slang = slang_corpus[slang_corpus['slang'] == sub_comment]\n",
    "            if len(sim_slang) != 0:\n",
    "                mode_word = sim_slang.formal.mode()[0]\n",
    "                corr_word = corr_word + 1\n",
    "                corr_comment.append(mode_word)\n",
    "                comment_found = True\n",
    "            else:\n",
    "                rat_words = []\n",
    "                for slang in slang_corpus['slang']:\n",
    "                    rat_word = nltk.edit_distance(sub_comment, slang)\n",
    "                    rat_words.append(rat_word)\n",
    "                min_rat = rat_words.index(min(rat_words))\n",
    "                value = str(slang_corpus.formal.loc[min_rat])\n",
    "                corr_words = corr_word + 1\n",
    "                corr_comment.append(value)\n",
    "                slang_words.append(sub_comment + \" = \" + value)\n",
    "                comment_found = True\n",
    "            if comment_found ==  False:\n",
    "                corr_comment.append(sub_comment)\n",
    "                slang_words.append(sub_comment + \" = \" + \"Not Found\")\n",
    "        tot_word = tot_word + 1\n",
    "    corr_comments.append(corr_comment)\n",
    "    print(\"Total kata yang sudah diproses : \" + str(tot_word))\n",
    "print(\"Total kata yang diperbaiki : \" + str(corr_word) + \" dari total semua kata : \" + str(tot_word))\n",
    "finish_time = time.time()\n",
    "print('Elapsed time: {}'.format(timedelta(seconds=finish_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#komentar yang sudah diperbaiki\n",
    "corr_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kata singkatan yang tidak ada pada slang corpus dan diolah dengan edit distance\n",
    "slang_words"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
